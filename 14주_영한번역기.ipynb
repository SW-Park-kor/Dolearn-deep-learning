{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 필요한 패키지 Import\n",
    "import tensorflow as tf  # 본 실습에서 사용한 tensorflow는 1.13.1 (또는 1.14.1) 버전임 \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata  # Unicode Character Database를 접근할 수 있도록 하는 모듈\n",
    "import re          # regular expression package\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "\n",
    "import plotly  # plotly는 3.10.0 버전을 사용함, Plotly Python Open Source Graphing Library\n",
    "import plotly.plotly as py\n",
    "\n",
    "# iplot: plotly 렌더링 모듈의 구버전\n",
    "from plotly.offline import init_notebook_mode, iplot \n",
    "\n",
    "#  init_notebook_mode: Jupyter Notebook 환경에서 Plotly를 사용하기 위해 init_notebook_mode를 호출\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "# graph_objs: scatter plot을 그리기 위해 사용하는 plotly의 모듈 중 하나\n",
    "\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotly.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './datasets/14_영한번역_소스코드/kor-eng/kor.txt' # 데이터셋의 위치를 명시함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Some water, please.\\t물 좀 주세요, 제발.',\n",
       " 'The light went out.\\t전등이 꺼졌다.',\n",
       " 'The night was cold.\\t그날 밤은 추웠어.',\n",
       " 'Tie your shoelaces.\\t신발끈을 묶으세요.',\n",
       " 'We were retreating.\\t우리는 후퇴하고 있었다.',\n",
       " \"We've been worried.\\t계속 걱정했어.\",\n",
       " 'When will you come?\\t언제쯤 올거야?',\n",
       " 'Whose book is this?\\t이것은 누구의 책입니까?',\n",
       " 'Can I have this cup?\\t이 컵 가져도 돼요?',\n",
       " 'Do you like English?\\t영어 좋아해요?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# txt 파일에서 데이터를 읽고 정제함\n",
    "# 데이터셋을 읽고(read), 문자열의 양 끝에 존재하는 공백과 \\n 제거(strip) 및 \\n로 분리(split)하여 라인 생성\n",
    "lines = open(file_path, encoding='UTF-8').read().strip().split('\\n') \n",
    "# 각 라인을 \\t로 분리했을 때 처음 2개 string만 추출 \n",
    "lines = [l.split('\\t')[:2] for l in lines]\n",
    "# 영문장과 해당 번역문을 \\t로 묶어 하나의 라인 형성\n",
    "lines = [l[0] + '\\t' + l[1] for l in lines]\n",
    "# 200번째 라인부터 10개 라인 추출하여 출력\n",
    "lines[200:210] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "904"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수 문자와 숫자를 제거하기 위한 작업\n",
    "exclude = set(string.punctuation) # 모든 특수 문자를 exclude 변수에 저장\n",
    "remove_digits = str.maketrans('', '', string.digits) # 숫자로 구성된 Set을 remove_digits에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 문장을 전처리\n",
    "def preprocess_eng_sentence(sent):\n",
    "    '''영어 문장 전처리 함수'''\n",
    "    sent = sent.lower() # 영어 문자를 소문자로 변환\n",
    "    sent = re.sub(\"'\", '', sent) # \"'\"부호가 있으면 삭제함\n",
    "    sent = ''.join(ch for ch in sent if ch not in exclude) # 특수부호를 삭제\n",
    "    sent = sent.translate(remove_digits) # 모든 숫자를 제거함\n",
    "    sent = sent.strip() # 문자열의 양 끝에 존재하는 공백과 \\n 제거\n",
    "    sent = re.sub(\" +\", \" \", sent) # 특정 패턴의 문자열(\" +\")을 다른 문자열(\" \")로 바꾸기 \n",
    "    sent = '<start> ' + sent + ' <end>' # <start> 와 <end> token을 추가함\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 문장을 전처리\n",
    "def preprocess_kor_sentence(sent):\n",
    "    '''Function to preprocess Korean sentence'''\n",
    "    sent = re.sub(\"'\", '', sent) # \"'\"부호가 있으면 삭제함\n",
    "    sent = ''.join(ch for ch in sent if ch not in exclude) # 특수부호를 삭제\n",
    "    sent = sent.strip() # 문자열의 양 끝에 존재하는 공백과 \\n 제거\n",
    "    sent = re.sub(\" +\", \" \", sent) # 특정 패턴의 문자열(\" +\")을 다른 문자열(\" \")로 바꾸기\n",
    "    sent = '<start> ' + sent + ' <end>' # <start> 와 <end> token을 추가함\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<start> some water please <end>', '<start> 물 좀 주세요 제발 <end>'],\n",
       " ['<start> the light went out <end>', '<start> 전등이 꺼졌다 <end>'],\n",
       " ['<start> the night was cold <end>', '<start> 그날 밤은 추웠어 <end>'],\n",
       " ['<start> tie your shoelaces <end>', '<start> 신발끈을 묶으세요 <end>'],\n",
       " ['<start> we were retreating <end>', '<start> 우리는 후퇴하고 있었다 <end>'],\n",
       " ['<start> weve been worried <end>', '<start> 계속 걱정했어 <end>'],\n",
       " ['<start> when will you come <end>', '<start> 언제쯤 올거야 <end>'],\n",
       " ['<start> whose book is this <end>', '<start> 이것은 누구의 책입니까 <end>'],\n",
       " ['<start> can i have this cup <end>', '<start> 이 컵 가져도 돼요 <end>'],\n",
       " ['<start> do you like english <end>', '<start> 영어 좋아해요 <end>']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제된 영어와 한국어 문장을 쌍으로 생성\n",
    "sent_pairs = []\n",
    "for line in lines:\n",
    "    sent_pair = []\n",
    "    eng, mar = line.split('\\t') # 영문장 및 번역문 쌍을 각각 eng와 mar로 나누어 저장 \n",
    "    \n",
    "    # 영어 문장\n",
    "    eng = preprocess_eng_sentence(eng) # 영문장 eng 전처리\n",
    "    sent_pair.append(eng) # 전처리 된 영문장을 sent_pair 리스트의 첫번째 원소로 저장\n",
    "    # 한글 문장\n",
    "    mar = preprocess_kor_sentence(mar) # 번역문 mar 전처리\n",
    "    sent_pair.append(mar) # 전처리 된 번역문을 sent_pair 리스트의 두번째 원소로 저장\n",
    "    sent_pairs.append(sent_pair) # 전처리가 끝난 영문장과 번역문으로 이루어진 리스트를 sent_pairs 리스트에 저장\n",
    "\n",
    "# 200번째 라인부터 10개 라인 추출\n",
    "sent_pairs[200:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어에서 인텍스의 매핑과 인텍스에서 단어의 매핑을 생성하는 클래스 정의\n",
    "# (e.g., 5 -> \"dad\") for each language,\n",
    "class LanguageIndex():\n",
    "    def __init__(self, lang):\n",
    "        # 변수 및 형태(타입) 선언\n",
    "        self.lang = lang # lang \n",
    "        self.word2idx = {} # 단어-인덱스 : 딕셔너리 타입\n",
    "        self.idx2word = {} # 인덱스-단어 : 딕셔너리 타입\n",
    "        self.vocab = set() # 사전 구축(데이터의 중복을 허용하지 않으며 저장되는 데이터에 대한 순서가 없음) \n",
    "\n",
    "        self.create_index() # 인덱스 생성 함수 선언\n",
    "    \n",
    "    # 단어의 Index를 생성\n",
    "    def create_index(self):\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' ')) # 띄어쓰기(' ')로 나뉘어 생성된 단어(어절)를 사전에 추가\n",
    "\n",
    "        self.vocab = sorted(self.vocab) # 알파벳 순서대로 사전(단어) 정렬\n",
    "\n",
    "        self.word2idx['<pad>'] = 0 # <pad> 단어에 인덱스 0을 지정 (예시: word2idx = {'<pad>': 0})\n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1 # 위에서 생성된 사전(vocab)의 각 단어에 인덱스 생성(1부터)\n",
    "\n",
    "        for word, index in self.word2idx.items():\n",
    "            # word2idx의 각 단어에 지정된 인덱스를 idx2word에 인덱스: 단어 형식으로 지정\n",
    "            # 예시: idx2word = {0:'<pad>'}\n",
    "            self.idx2word[index] = word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서의 최대 길이를 계산하는 함수\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터와 타겟 데이터에 대해 Indexing 작업을 진행\n",
    "# 패딩(Padding) 작업 진행\n",
    "def load_dataset(pairs, num_examples):\n",
    "    # pairs => already created cleaned input, output pairs\n",
    "\n",
    "    # index language using the class defined above    \n",
    "    inp_lang = LanguageIndex(en for en, ma in pairs)\n",
    "    targ_lang = LanguageIndex(ma for en, ma in pairs)\n",
    "    \n",
    "    # Vectorize the input and target languages\n",
    "    \n",
    "    # 영어 문장\n",
    "    # \"<start> no way <end>\" => [2, 656, 1079, 1]\n",
    "    input_tensor = [[inp_lang.word2idx[s] for s in en.split(' ')] for en, ma in pairs]\n",
    "    \n",
    "    # 한국어 문장\n",
    "    target_tensor = [[targ_lang.word2idx[s] for s in ma.split(' ')] for en, ma in pairs]\n",
    "    \n",
    "    # 입력 텐서(영어 문장), 출력 텐서(한국어 문장)에서 최대 길이를 각각 계산함 \n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    # 텐서의 최대 길이에 맞추어 패딩(Padding)을 추가하는 작업\n",
    "    # 해당 단어에 대해 Index 번호를 추가, 없는 부분에 0 값으로 패딩 \n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, # 영어문장\n",
    "                                                                 maxlen=max_length_inp, # 영어문장 텐서의 최대 길이 \n",
    "                                                                 padding='post') # 'pre' 또는 'post': pre를 쓰면 시퀀스 앞에 패딩을 하고 post를 쓰면 시퀀스 뒤에 패딩\n",
    "    \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, # 번역문\n",
    "                                                                  maxlen=max_length_tar, # 번역문 텐서의 최대 길이 \n",
    "                                                                  padding='post') # 'pre' 또는 'post': pre를 쓰면 시퀀스 앞에 패딩을 하고 post를 쓰면 시퀀스 뒤에 패딩\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 1099    1 ...    0    0    0]\n",
      " [   2  436    1 ...    0    0    0]\n",
      " [   2  656 1079 ...    0    0    0]\n",
      " ...\n",
      " [   2 1002  665 ...    0    0    0]\n",
      " [   2  468  981 ...  997  822    1]\n",
      " [   2  497  660 ... 1129    1    0]]\n",
      "The max length of Input Tensor:  19\n",
      "The max length of Output Tensor:  17\n"
     ]
    }
   ],
   "source": [
    "# load_dataset() 함수 호출\n",
    "input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(sent_pairs, len(lines))\n",
    "\n",
    "print(input_tensor)\n",
    "print(\"The max length of Input Tensor: \", max_length_inp)\n",
    "print(\"The max length of Output Tensor: \", max_length_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(813, 813, 91, 91)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련용 데이터와 테스트 데이터를 9대1로 분할\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, \\\n",
    "                                                                                                target_tensor, test_size=0.1\n",
    "                                                                                             , random_state = 101)\n",
    "\n",
    "# 분할된 데이터 개수를 보여줌\n",
    "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ShuffleDataset shapes: ((19,), (17,)), types: (tf.int32, tf.int32)>\n",
      "<BatchDataset shapes: ((64, 19), (64, 17)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼 파라미터 설정\n",
    "BUFFER_SIZE = len(input_tensor_train) # 학습데이터 사이즈\n",
    "BATCH_SIZE = 64 # batch 사이즈\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE # batch 수\n",
    "'''\n",
    "N_BATCH는 각각의 batch 그룹에 들어가는 배치의 수\n",
    "BUFFER_SIZE = 813, BATCH_SIZE = 64로 설정하였으므로 813//64 = 12\n",
    "N_BATCH = 12, 즉, 하나의 배치그룹은 12개의 배치를 가짐\n",
    "'''\n",
    "embedding_dim = 256 # 임베딩 차원\n",
    "units = 1024 # 신경망 유닛 수\n",
    "\n",
    "# 입력 데이터(영어)의 전체 단어의 개수\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "# 출력 데이터(한글)의 전체 단어의 개수\n",
    "vocab_tar_size = len(targ_lang.word2idx)\n",
    "\n",
    "# tf.data.Dataset을 생성하는 함수로, numpy 배열이 있고 그걸 tensorflow로 넣는 기본 케이스\n",
    "# 데이터를 특성(feature)과 라벨(label)로 나누어 사용하는 경우처럼, 한 개 이상의 numpy 배열을 넣을 수 있음\n",
    "# 해당 방법을 사용하면 학습 속도가 빨라지는 장점이 있음\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "print(dataset)\n",
    "\n",
    "# 배치 데이터를 생성\n",
    "# 연속의 원소를 batch에 batch_size만큼 합치는 작업\n",
    "# 마지막 batch가 batch_size보다 작은 경우 drop\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) \n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    # 정의한  GRU 구조.\n",
    "    return tf.keras.layers.GRU(units, # 결과값 차원 수(dimensionality of the output space)\n",
    "                               return_sequences=True, # 마지막 시퀀스를 출력할 것인지, 아니면 전체 시퀀스를 출력할 것인지 여부\n",
    "                               return_state=True, # output 외에도 최후 state를 출력할 것인지 여부\n",
    "                               recurrent_activation='sigmoid', # recurrent 단계에서 activation function\n",
    "                               recurrent_initializer='glorot_uniform') # recurrent_kernel 가중치 행렬 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 클래스를 정의\n",
    "class Encoder(tf.keras.Model):\n",
    "    \n",
    "    # 전체 영어 단어의 길이, 임베딩 차원, 유닛 개수, 배치 사이즈를 입력으로 함\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz # 배치 사이즈\n",
    "        self.enc_units = enc_units # 유닛(unit) 개수\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # 임베딩 층, (영어 단어의 개수, 임베딩 차원)\n",
    "        self.gru = gru(self.enc_units) # GRU 층\n",
    "    \n",
    "    # 인코더 호출 함수\n",
    "    # 해당 함수는 임베딩 층과 GRU 층으로 구성됨\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)        \n",
    "        return output, state\n",
    "    \n",
    "    # Hidden state를 초기화\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 클래스를 정의\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz # 배치 사이즈\n",
    "        self.dec_units = dec_units # 유닛(unit) 개수\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # 임베딩 층\n",
    "        self.gru = gru(self.dec_units) # GRU 층\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size) # 전체 한국어 단어의 개수\n",
    "        \n",
    "        # Attention에서 사용함\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units) # encoder ouptput 가중치\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units) # RNN 셀 각각의 output을 입력값으로 취하는 Feed-Forward Fully Connected Layer\n",
    "        self.V = tf.keras.layers.Dense(1) # context vector\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        '''\n",
    "        해당 레이어의 output을 각 RNN 셀의 Score로 결정.\n",
    "        출력된 score에 Softmax를 취하여 0~1값으로 변환하고 각각의 attention weight로 변환.\n",
    "        attention weight와 hidden state를 곱해서 Context vector 획득 후, 디코더에 input으로 들어감.\n",
    "        결과가 정답이 아니면 attention weight를 backpropagate방식으로 다시 조절하면서 학습하며, \n",
    "        따라서 디코더의 hidden state가 attention weight 계산에 영향을 줌.\n",
    "        '''\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # 스코어를 구하기 위해 차원(1) 추가\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # tanh(FC(EO) + FC(H))을 self.V에 적용하여 마지막 축에 1을 가짐\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # 소프트맥스 스코어 산출\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        # 가중치 적용\n",
    "        context_vector = attention_weights * enc_output\n",
    "        # 텐서의 차원들을 탐색하며 개체들의 총합을 계산함\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # 임베딩 후 x의 shape == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # 임베딩 된 x와 context_vector를 모두 붙이고 난 뒤 x의 shape == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # LSTM의 변형인 GRU(Gated Recurrent Unit)로 x를 보냄\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        # hidden state를 0으로 채워 초기화\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더와 디코더 객체를 생성\n",
    "# 'vocab_inp_size' => 전체 영어 단어의 개수, 'vocab_tar_size' => 전체 한국어 단어의 개수\n",
    "# 'embedding_dim' => 256, 'units' => 1024, 'BATCH_SIZE' => 64\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 함수\n",
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "# 손실함수 정의\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0) # 원소 단위로 비교 연산을 만족하면 True 반환, 만족하지 않으면 False 반환함\n",
    "    # softmax 산출 후 cross_entropy를 구하는 함수\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크 포인트 생성\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.3794\n",
      "Epoch 1 Loss 2.2221\n",
      "Time taken for 1 epoch 64.12617707252502 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.2513\n",
      "Epoch 2 Loss 1.9918\n",
      "Time taken for 1 epoch 69.75529956817627 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.8079\n",
      "Epoch 3 Loss 1.8694\n",
      "Time taken for 1 epoch 63.175485134124756 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.7951\n",
      "Epoch 4 Loss 1.7686\n",
      "Time taken for 1 epoch 51.86296486854553 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.7496\n",
      "Epoch 5 Loss 1.6851\n",
      "Time taken for 1 epoch 67.77093362808228 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.6983\n",
      "Epoch 6 Loss 1.6285\n",
      "Time taken for 1 epoch 65.52886605262756 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.7053\n",
      "Epoch 7 Loss 1.5941\n",
      "Time taken for 1 epoch 55.870238065719604 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.5944\n",
      "Epoch 8 Loss 1.5417\n",
      "Time taken for 1 epoch 59.78965210914612 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.5488\n",
      "Epoch 9 Loss 1.5166\n",
      "Time taken for 1 epoch 63.44730353355408 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.5154\n",
      "Epoch 10 Loss 1.4739\n",
      "Time taken for 1 epoch 71.14693236351013 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.3989\n",
      "Epoch 11 Loss 1.4327\n",
      "Time taken for 1 epoch 72.4179482460022 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.4089\n",
      "Epoch 12 Loss 1.3961\n",
      "Time taken for 1 epoch 69.22612881660461 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 1.3553\n",
      "Epoch 13 Loss 1.3614\n",
      "Time taken for 1 epoch 72.82700967788696 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 1.2125\n",
      "Epoch 14 Loss 1.3003\n",
      "Time taken for 1 epoch 73.0801191329956 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 1.2801\n",
      "Epoch 15 Loss 1.2576\n",
      "Time taken for 1 epoch 72.95527458190918 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 1.0553\n",
      "Epoch 16 Loss 1.2163\n",
      "Time taken for 1 epoch 70.36977410316467 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 1.2154\n",
      "Epoch 17 Loss 1.1613\n",
      "Time taken for 1 epoch 72.75094652175903 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 1.1388\n",
      "Epoch 18 Loss 1.0989\n",
      "Time taken for 1 epoch 70.8371651172638 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.9311\n",
      "Epoch 19 Loss 1.0374\n",
      "Time taken for 1 epoch 70.15931987762451 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.8815\n",
      "Epoch 20 Loss 0.9747\n",
      "Time taken for 1 epoch 66.13165187835693 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "# Epoch 만큼 반복해 모델을 훈련함\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # 인코더의 히든 상태를 초기화\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    # 전체 손실을 저장하는 변수를 정의하고 0을 초기값으로 설정\n",
    "    total_loss = 0\n",
    "    \n",
    "    # 반복문을 사용해 배치 사이즈만큼의 영어 문장과 한국어 문장을 가져옴\n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        # GradientTape는 자동 미분을 계산하는 API\n",
    "        # 수동으로 그래디언트를 게산해 가중치를 업데이트하기 위함\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # 배치 사이즈 만큼인 영어 문장과 hidden 상태값을 사용해 인코더를 호출\n",
    "            # 인코더 출력과 인코더 히든을 반환\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            # 디코더의 입력을 준비 (디코더 모델의 첫번째 스텝의 입력을 '<start>'의 index 번호로 설정함)\n",
    "            # shape은 (64, 1)\n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)    \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            # 디코더를 step by step으로 호출해 예측 값에 대한 손실을 계산함\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                \n",
    "                # 인코딩 출력을 디코더에 입력함\n",
    "                # 디코더의 입력, 히든 값, 인코더의 출력을 사용해 디코더를 호출\n",
    "                # 그 다음 예측 값과 히든 값을 반환\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                # 타겟 값과 예측 값을 비교해 손실 값을 계산\n",
    "                # 게산된 손실 값을 누적함\n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # 그 다음 디코더의 입력을 설정\n",
    "                # 현재 스탭의 타겟으로 설정함 (예: target: [<start>, a,, b, c, d] 일 때 \n",
    "                # 디코더의 첫번째 스텝의 입력: <start>, 두 번째 입력: a, 세 번째 입력: b, )\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        # 배치 손실을 계산\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        # 전체 손실의 누적 합\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        # 업데이트를 하기 위한 인코더와 디코더의 가중치를 가지고 옴\n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        # 손실을 사용해 기울기 값을 계산함\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        # 기울기 값을 사용해 가중치를 업데이트함\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        # 100번째 배치마다 출력\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # 매 Epoch마다 모델을 체크포인트 폴더에 저장\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x20a00069130>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inputs, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n",
    "    \"\"\"훈련한 모델을 사용해 번역 작업을 진행하는 함수\"\"\"\n",
    "    \n",
    "    # heat map을 그리기 위한 matrix 초기화\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    # 입력 inputs의 인덱스를 단어로 디코딩해 문장을 생성함\n",
    "    # 예: [2, 1099, 1, 0, 0 ,..] => '<start> who <end>'\n",
    "    sentence = ''\n",
    "    for i in inputs[0]:\n",
    "        if i == 0:\n",
    "            break\n",
    "        sentence = sentence + inp_lang.idx2word[i] + ' '\n",
    "    \n",
    "    # 마지막에 추가된 스페이스를 지움 \n",
    "    sentence = sentence[:-1]\n",
    "    \n",
    "    #NumPy ndarray 자료형을 변환\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    # 모든 원소의 값이 0인 텐서를 생성\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    \n",
    "    # inputs를 encoder에 입력\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n",
    "\n",
    "    # Step-by-step으로 decoder를 실행해 문장을 번역함\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # attention weight heat map를 그리기 위해 매 step의 attention 가중치를 저장함\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        # 텐서 안에서 predictions[0]을 따라 가장 큰 값의 인덱스를 찾기\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        # decoder가 예측한 인덱스를 단어로 디코딩함\n",
    "        result += targ_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        # docoder 실행 중 '<end>'가 나타나면 문장이 끝이므로 실행 중지\n",
    "        if targ_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # 예측한 출력을 decoder의 다음 step의 입력으로 사용하기 위해 dimension을 expand 함\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_random_val_sentence():\n",
    "    \"\"\"테스트셋에서 랜덤으로 문장을 선택해 번역하고 attention heatmap을 그리는 함수\"\"\"\n",
    "    \n",
    "    actual_sent = ''\n",
    "    \n",
    "    # 테스트셋에서 랜덤으로 인코딩 된 문장을 선택\n",
    "    k = np.random.randint(len(input_tensor_val))\n",
    "    random_input = input_tensor_val[k]\n",
    "    random_output = target_tensor_val[k]\n",
    "    random_input = np.expand_dims(random_input,0)\n",
    "    \n",
    "    # evaluate 함수를 사용해 번역 작업을 진행함\n",
    "    result, sentence, attention_plot = evaluate(random_input, encoder, decoder, inp_lang, targ_lang, max_length_inp,\\\n",
    "                                                max_length_targ)\n",
    "    print('Input: {}'.format(sentence[8:-6]))  # 입력 문장의 <start>와 <end>를 제거하고 출력\n",
    "    print('Predicted translation: {}'.format(result[:-6]))  # 번역한 문장에서 <end>를 제거하고 출력\n",
    "    # 랜덤으로 선택한 입력 문장의 정답 레이블을 인텍스에서 단어로 디코딩함\n",
    "    for i in random_output:\n",
    "        if i == 0:\n",
    "            break\n",
    "        actual_sent = actual_sent + targ_lang.idx2word[i] + ' '\n",
    "    actual_sent = actual_sent[8:-7]   # 타깃 문장의 <start>와 <end>를 제거하고 출력\n",
    "    print('Actual translation: {}'.format(actual_sent))\n",
    "    \n",
    "    # attention weight \n",
    "    attention_plot = attention_plot[:len(result.split(' '))-2, 1:len(sentence.split(' '))-1]\n",
    "    \n",
    "    # 영문장 및 번역문 토큰 리스트 지정\n",
    "    sentence, result = sentence.split(' '), result.split(' ')\n",
    "    sentence = sentence[1:-1]\n",
    "    result = result[:-2]\n",
    "    \n",
    "    # plotly를 사용해 attention weight의 heat map를 생성함\n",
    "    trace = go.Heatmap(z = attention_plot, x = sentence, y = result, colorscale='Reds')\n",
    "    data=[trace]\n",
    "    iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: how many times do i need to repeat it\n",
      "Predicted translation: 톰은 메리가 피아노 치는 것을 믿을 수 없었다 \n",
      "Actual translation: 몇 번을 반복해야 해\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": "Reds",
         "type": "heatmap",
         "uid": "c947511f-17fb-4606-ad91-6d01f1137283",
         "x": [
          "how",
          "many",
          "times",
          "do",
          "i",
          "need",
          "to",
          "repeat",
          "it"
         ],
         "y": [
          "톰은",
          "메리가",
          "피아노",
          "치는",
          "것을",
          "믿을",
          "수",
          "없었다"
         ],
         "z": [
          [
           3.1942308851284906e-05,
           0.0003983050992246717,
           4.224732765578665e-05,
           0.00018727562564890832,
           0.00022494173026643693,
           2.076524651783984e-05,
           3.6894539334753063e-06,
           7.919391464383807e-06,
           1.75117911567213e-05
          ],
          [
           0.00010433037095936015,
           0.0004796250141225755,
           9.152424172498286e-05,
           0.00019971524307038635,
           0.0002212873805547133,
           3.9230071706697345e-05,
           9.689827493275516e-06,
           1.23483396237134e-05,
           1.654629704717081e-05
          ],
          [
           5.632418766414113e-14,
           7.644701928780084e-13,
           5.1433432979221955e-14,
           1.836941243647111e-13,
           2.1864118588538206e-13,
           1.6778348797667243e-14,
           2.9986768395397516e-15,
           4.394686451227904e-15,
           6.931240537712312e-15
          ],
          [
           4.5435508654045886e-13,
           5.3843353386984205e-12,
           4.840746401814366e-13,
           1.7336273475804242e-12,
           2.0328166233651856e-12,
           2.0288027442923184e-13,
           4.579567993811921e-14,
           7.270865767100565e-14,
           1.2054031004685856e-13
          ],
          [
           3.8027111841365535e-13,
           2.4753033824043147e-12,
           3.9643662701552285e-13,
           1.0511344399055655e-12,
           1.1794172409540504e-12,
           2.1118577806650274e-13,
           7.440757537023612e-14,
           1.0497313874266792e-13,
           1.5408450882402336e-13
          ],
          [
           2.317888103675614e-09,
           1.7053890744023192e-08,
           2.1777823988600176e-09,
           6.012683684275544e-09,
           6.856556655776558e-09,
           8.965500564173112e-10,
           2.146653271806187e-10,
           2.9505647924921163e-10,
           4.321562552256353e-10
          ],
          [
           5.759963361308407e-13,
           2.4155991876911864e-12,
           6.057333996369041e-13,
           1.2614339911162786e-12,
           1.402670297620534e-12,
           4.0034780503760137e-13,
           1.9746773389627686e-13,
           2.554647303160823e-13,
           3.425982913161929e-13
          ],
          [
           1.1356786344673964e-10,
           8.509041249382676e-10,
           1.224205598004957e-10,
           3.4618360955640526e-10,
           4.0555414582144067e-10,
           6.347220665725573e-11,
           1.9706623485826746e-11,
           2.9611032376086754e-11,
           4.69155998772397e-11
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"cecdd79a-055d-4ea7-b3b5-4b87f5471f1c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"cecdd79a-055d-4ea7-b3b5-4b87f5471f1c\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'cecdd79a-055d-4ea7-b3b5-4b87f5471f1c',\n",
       "                        [{\"colorscale\": \"Reds\", \"type\": \"heatmap\", \"uid\": \"c947511f-17fb-4606-ad91-6d01f1137283\", \"x\": [\"how\", \"many\", \"times\", \"do\", \"i\", \"need\", \"to\", \"repeat\", \"it\"], \"y\": [\"\\ud1b0\\uc740\", \"\\uba54\\ub9ac\\uac00\", \"\\ud53c\\uc544\\ub178\", \"\\uce58\\ub294\", \"\\uac83\\uc744\", \"\\ubbff\\uc744\", \"\\uc218\", \"\\uc5c6\\uc5c8\\ub2e4\"], \"z\": [[3.1942308851284906e-05, 0.0003983050992246717, 4.224732765578665e-05, 0.00018727562564890832, 0.00022494173026643693, 2.076524651783984e-05, 3.6894539334753063e-06, 7.919391464383807e-06, 1.75117911567213e-05], [0.00010433037095936015, 0.0004796250141225755, 9.152424172498286e-05, 0.00019971524307038635, 0.0002212873805547133, 3.9230071706697345e-05, 9.689827493275516e-06, 1.23483396237134e-05, 1.654629704717081e-05], [5.632418766414113e-14, 7.644701928780084e-13, 5.1433432979221955e-14, 1.836941243647111e-13, 2.1864118588538206e-13, 1.6778348797667243e-14, 2.9986768395397516e-15, 4.394686451227904e-15, 6.931240537712312e-15], [4.5435508654045886e-13, 5.3843353386984205e-12, 4.840746401814366e-13, 1.7336273475804242e-12, 2.0328166233651856e-12, 2.0288027442923184e-13, 4.579567993811921e-14, 7.270865767100565e-14, 1.2054031004685856e-13], [3.8027111841365535e-13, 2.4753033824043147e-12, 3.9643662701552285e-13, 1.0511344399055655e-12, 1.1794172409540504e-12, 2.1118577806650274e-13, 7.440757537023612e-14, 1.0497313874266792e-13, 1.5408450882402336e-13], [2.317888103675614e-09, 1.7053890744023192e-08, 2.1777823988600176e-09, 6.012683684275544e-09, 6.856556655776558e-09, 8.965500564173112e-10, 2.146653271806187e-10, 2.9505647924921163e-10, 4.321562552256353e-10], [5.759963361308407e-13, 2.4155991876911864e-12, 6.057333996369041e-13, 1.2614339911162786e-12, 1.402670297620534e-12, 4.0034780503760137e-13, 1.9746773389627686e-13, 2.554647303160823e-13, 3.425982913161929e-13], [1.1356786344673964e-10, 8.509041249382676e-10, 1.224205598004957e-10, 3.4618360955640526e-10, 4.0555414582144067e-10, 6.347220665725573e-11, 1.9706623485826746e-11, 2.9611032376086754e-11, 4.69155998772397e-11]]}],\n",
       "                        {},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('cecdd79a-055d-4ea7-b3b5-4b87f5471f1c');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_random_val_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tom doesnt know why im here yet\n",
      "Predicted translation: 톰은 메리가 나의 쉽지는 않아 \n",
      "Actual translation: 톰은 제가 왜 여기 있는지 아직 모릅니다\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": "Reds",
         "type": "heatmap",
         "uid": "0cf9a891-904a-4f81-a3fb-0bfe79585ff3",
         "x": [
          "tom",
          "doesnt",
          "know",
          "why",
          "im",
          "here",
          "yet"
         ],
         "y": [
          "톰은",
          "메리가",
          "나의",
          "쉽지는",
          "않아"
         ],
         "z": [
          [
           1.812105551834975e-06,
           1.3156859495211393e-06,
           1.651602445917888e-07,
           1.4112731427928793e-08,
           2.1724309462456404e-08,
           2.572029416114674e-07,
           0.0007922073127701879
          ],
          [
           3.713116484505008e-06,
           1.6522708392585628e-06,
           2.425422849228198e-07,
           2.6749319204100175e-08,
           2.5411319271029242e-08,
           1.1395689369919637e-07,
           3.1654391932534054e-05
          ],
          [
           5.604939302363465e-16,
           2.713526691857499e-16,
           4.733027541610428e-17,
           8.281943223937114e-18,
           9.426651176689035e-18,
           3.828874795209136e-17,
           1.9839964632110962e-14
          ],
          [
           1.4127267598142818e-15,
           8.758445611969186e-16,
           1.8024637447933877e-16,
           3.694620403941574e-17,
           4.485078498636433e-17,
           1.8279034529218845e-16,
           9.764818755019292e-14
          ],
          [
           6.041839945014615e-11,
           4.910707257499425e-11,
           2.969373358308047e-11,
           1.8704470264307993e-11,
           1.9095150807779682e-11,
           2.689183077608792e-11,
           1.8632127785078723e-10
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"be3aed4e-8a41-440c-8784-6669ebd7f3db\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"be3aed4e-8a41-440c-8784-6669ebd7f3db\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'be3aed4e-8a41-440c-8784-6669ebd7f3db',\n",
       "                        [{\"colorscale\": \"Reds\", \"type\": \"heatmap\", \"uid\": \"0cf9a891-904a-4f81-a3fb-0bfe79585ff3\", \"x\": [\"tom\", \"doesnt\", \"know\", \"why\", \"im\", \"here\", \"yet\"], \"y\": [\"\\ud1b0\\uc740\", \"\\uba54\\ub9ac\\uac00\", \"\\ub098\\uc758\", \"\\uc27d\\uc9c0\\ub294\", \"\\uc54a\\uc544\"], \"z\": [[1.812105551834975e-06, 1.3156859495211393e-06, 1.651602445917888e-07, 1.4112731427928793e-08, 2.1724309462456404e-08, 2.572029416114674e-07, 0.0007922073127701879], [3.713116484505008e-06, 1.6522708392585628e-06, 2.425422849228198e-07, 2.6749319204100175e-08, 2.5411319271029242e-08, 1.1395689369919637e-07, 3.1654391932534054e-05], [5.604939302363465e-16, 2.713526691857499e-16, 4.733027541610428e-17, 8.281943223937114e-18, 9.426651176689035e-18, 3.828874795209136e-17, 1.9839964632110962e-14], [1.4127267598142818e-15, 8.758445611969186e-16, 1.8024637447933877e-16, 3.694620403941574e-17, 4.485078498636433e-17, 1.8279034529218845e-16, 9.764818755019292e-14], [6.041839945014615e-11, 4.910707257499425e-11, 2.969373358308047e-11, 1.8704470264307993e-11, 1.9095150807779682e-11, 2.689183077608792e-11, 1.8632127785078723e-10]]}],\n",
       "                        {},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('be3aed4e-8a41-440c-8784-6669ebd7f3db');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_random_val_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: i hope that i can do it\n",
      "Predicted translation: 나는 톰에게 내 마지막 진행되었어요 \n",
      "Actual translation: 내가 할 수 있길 바라\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "colorscale": "Reds",
         "type": "heatmap",
         "uid": "4e9b6c4d-1e36-40f4-bec7-11ac24cd3792",
         "x": [
          "i",
          "hope",
          "that",
          "i",
          "can",
          "do",
          "it"
         ],
         "y": [
          "나는",
          "톰에게",
          "내",
          "마지막",
          "진행되었어요"
         ],
         "z": [
          [
           0.00011644980259006843,
           0.0006753550260327756,
           0.00029910108423791826,
           0.00026197059196420014,
           0.0001534357579657808,
           0.00033219685428775847,
           0.00017247593495994806
          ],
          [
           0.00019864369824063033,
           0.0006194856250658631,
           0.0003601661301217973,
           0.0003355587541591376,
           0.00022608974541071802,
           0.0003425100876484066,
           0.00020466101705096662
          ],
          [
           2.7150997006231137e-09,
           1.880810351906348e-08,
           7.379040933841452e-09,
           6.506915006809777e-09,
           3.4196288023480292e-09,
           6.871794244744933e-09,
           2.9449200855680147e-09
          ],
          [
           1.619393970031524e-07,
           7.175063956310623e-07,
           3.4995559872186277e-07,
           3.1699542546448356e-07,
           1.9323951505612058e-07,
           3.3375229691046115e-07,
           1.750765363794926e-07
          ],
          [
           3.21226106762937e-15,
           3.234216415732086e-14,
           1.0414914678564483e-14,
           8.805150585164314e-15,
           4.4956870833749805e-15,
           1.0945131892557258e-14,
           4.543062483147861e-15
          ]
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"bc0665da-2f7c-4b7b-ba55-c91128731488\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"bc0665da-2f7c-4b7b-ba55-c91128731488\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        'bc0665da-2f7c-4b7b-ba55-c91128731488',\n",
       "                        [{\"colorscale\": \"Reds\", \"type\": \"heatmap\", \"uid\": \"4e9b6c4d-1e36-40f4-bec7-11ac24cd3792\", \"x\": [\"i\", \"hope\", \"that\", \"i\", \"can\", \"do\", \"it\"], \"y\": [\"\\ub098\\ub294\", \"\\ud1b0\\uc5d0\\uac8c\", \"\\ub0b4\", \"\\ub9c8\\uc9c0\\ub9c9\", \"\\uc9c4\\ud589\\ub418\\uc5c8\\uc5b4\\uc694\"], \"z\": [[0.00011644980259006843, 0.0006753550260327756, 0.00029910108423791826, 0.00026197059196420014, 0.0001534357579657808, 0.00033219685428775847, 0.00017247593495994806], [0.00019864369824063033, 0.0006194856250658631, 0.0003601661301217973, 0.0003355587541591376, 0.00022608974541071802, 0.0003425100876484066, 0.00020466101705096662], [2.7150997006231137e-09, 1.880810351906348e-08, 7.379040933841452e-09, 6.506915006809777e-09, 3.4196288023480292e-09, 6.871794244744933e-09, 2.9449200855680147e-09], [1.619393970031524e-07, 7.175063956310623e-07, 3.4995559872186277e-07, 3.1699542546448356e-07, 1.9323951505612058e-07, 3.3375229691046115e-07, 1.750765363794926e-07], [3.21226106762937e-15, 3.234216415732086e-14, 1.0414914678564483e-14, 8.805150585164314e-15, 4.4956870833749805e-15, 1.0945131892557258e-14, 4.543062483147861e-15]]}],\n",
       "                        {},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('bc0665da-2f7c-4b7b-ba55-c91128731488');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_random_val_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
