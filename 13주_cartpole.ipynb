{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import gym  # 강화학습 환경을 제공해 주는 gym 라이브러리\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "EPISODES = 50  # 게임 진행 회수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.gamma = 0.99  # 감가율\n",
    "        self.learning_rate = 0.001  # 학습률\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = 64\n",
    "        self.train_threshold = 1000\n",
    "\n",
    "        self.memory = deque(maxlen=2000)  # DQN 알고리즘에서 사용할 리플레이 메모리\n",
    "\n",
    "        self.model = self.build_model()  # 정책 신경망 생성\n",
    "        self.target_model = self.build_model()  # 타깃 신경망 생성\n",
    "\n",
    "        self.update_target_model()  # 모델 업데이트\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"정책 모델 생성\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, input_shape=(self.state_size,), activation='relu'))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"타깃 모델 업데이트\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"e-greedy 정책으로 행동하는 함수 구현\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:  # 랜덤으로 생성한 숫자가 엡실론보다 작으면\n",
    "            return random.randrange(self.action_size)  # 랜덤으로 행동\n",
    "        else:\n",
    "            q_value = self.model.predict(state)  # 그렇지 않으면 Q 네트워크를 통해 행동\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"에이전트가 탐험과정에서 얻은 상태, 보상 등을 메모리에 저장\"\"\"\n",
    "        sample = (state, action, reward, next_state, done)\n",
    "        self.memory.append(sample)\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"정책 신경망 훈련\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:  # 엡실론이 정해진 최솟값에 도달하지 않았을 경우에\n",
    "            self.epsilon *= self.epsilon_decay  # 감가율을 사용해 엡실론 값을 줄임\n",
    "        \n",
    "        # 리플레이 메모리에서 배치 사이즈만큼의 샘플을 무작위로 추출\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)  \n",
    "        \n",
    "        states = np.zeros((self.batch_size, self.state_size))\n",
    "        next_states = np.zeros((self.batch_size, self.state_size))\n",
    "        actions, rewards, dones = [], [], []\n",
    "\n",
    "        # 샘플에서 상태, 행동, 보상 등을 추출해 배열에 저장\n",
    "        for i in range(self.batch_size):\n",
    "            states[i] = mini_batch[i][0]\n",
    "            actions.append(mini_batch[i][1])\n",
    "            rewards.append(mini_batch[i][2])\n",
    "            next_states[i] = mini_batch[i][3]\n",
    "            dones.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(states)  # 현재 상태의 Q 함수\n",
    "        y = self.target_model.predict(next_states)  # 다음 상태의 타깃 모델의 Q 함수\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                target[i][actions[i]] = rewards[i] + self.gamma * (np.amax(y[i]))\n",
    "\n",
    "        self.model.fit(states, target, batch_size=self.batch_size, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0/50, Score: 1.0, time: 0.59s\n",
      "Episode: 1/50, Score: 18.0, time: 0.51s\n",
      "Episode: 2/50, Score: 2.0, time: 0.21s\n",
      "Episode: 3/50, Score: 6.0, time: 0.28s\n",
      "Episode: 4/50, Score: 12.0, time: 0.39s\n",
      "Episode: 5/50, Score: 0.0, time: 0.18s\n",
      "Episode: 6/50, Score: 12.0, time: 0.38s\n",
      "Episode: 7/50, Score: 41.0, time: 0.88s\n",
      "Episode: 8/50, Score: 45.0, time: 0.96s\n",
      "Episode: 9/50, Score: -1.0, time: 0.17s\n",
      "Episode: 10/50, Score: 0.0, time: 0.18s\n",
      "Episode: 11/50, Score: 14.0, time: 0.44s\n",
      "Episode: 12/50, Score: 23.0, time: 0.59s\n",
      "Episode: 13/50, Score: 21.0, time: 0.54s\n",
      "Episode: 14/50, Score: 21.0, time: 0.52s\n",
      "Episode: 15/50, Score: 2.0, time: 0.21s\n",
      "Episode: 16/50, Score: 16.0, time: 0.51s\n",
      "Episode: 17/50, Score: 4.0, time: 0.26s\n",
      "Episode: 18/50, Score: 6.0, time: 0.28s\n",
      "Episode: 19/50, Score: -1.0, time: 0.16s\n",
      "Episode: 20/50, Score: 12.0, time: 0.38s\n",
      "Episode: 21/50, Score: 8.0, time: 0.31s\n",
      "Episode: 22/50, Score: 16.0, time: 0.44s\n",
      "Episode: 23/50, Score: 8.0, time: 0.31s\n",
      "Episode: 24/50, Score: 15.0, time: 0.43s\n",
      "Episode: 25/50, Score: 29.0, time: 0.66s\n",
      "Episode: 26/50, Score: 14.0, time: 0.43s\n",
      "Episode: 27/50, Score: 7.0, time: 0.30s\n",
      "Episode: 28/50, Score: 1.0, time: 0.19s\n",
      "Episode: 29/50, Score: 7.0, time: 0.31s\n",
      "Episode: 30/50, Score: 45.0, time: 0.94s\n",
      "Episode: 31/50, Score: 33.0, time: 0.72s\n",
      "Episode: 32/50, Score: 3.0, time: 0.23s\n",
      "Episode: 33/50, Score: 6.0, time: 0.28s\n",
      "Episode: 34/50, Score: 25.0, time: 0.60s\n",
      "Episode: 35/50, Score: 26.0, time: 0.61s\n",
      "Episode: 36/50, Score: 7.0, time: 0.29s\n",
      "Episode: 37/50, Score: 1.0, time: 0.19s\n",
      "Episode: 38/50, Score: 19.0, time: 0.50s\n",
      "Episode: 39/50, Score: -1.0, time: 0.16s\n",
      "Episode: 40/50, Score: 0.0, time: 0.18s\n",
      "Episode: 41/50, Score: 11.0, time: 0.36s\n",
      "Episode: 42/50, Score: 12.0, time: 5.11s\n",
      "Episode: 43/50, Score: 1.0, time: 2.37s\n",
      "Episode: 44/50, Score: 5.0, time: 3.01s\n",
      "Episode: 45/50, Score: 1.0, time: 2.39s\n",
      "Episode: 46/50, Score: 48.0, time: 11.38s\n",
      "Episode: 47/50, Score: 7.0, time: 3.30s\n",
      "Episode: 48/50, Score: 20.0, time: 6.28s\n",
      "Episode: 49/50, Score: 10.0, time: 4.11s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    env = gym.make('CartPole-v1')  # gym 라이브러리를 통해 CartPole 환경 생성\n",
    "    state_size = env.observation_space.shape[0]  # CartPole 게임의 상태의 크기\n",
    "    action_size = env.action_space.n  # CartPole 게임의 가능한 행동 개수\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)  # DQN 에이전트 생성\n",
    "\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()  # 환경 초기화\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        tic = time.time()\n",
    "        while not done:\n",
    "            env.render()\n",
    "\n",
    "            action = agent.act(state)  # 상태 state에서 e-greedy 정책으로 행동 선택\n",
    "            \n",
    "            # 선택한 행동을 진행해 다음 상태, 보상 등을 얻음\n",
    "            next_state, reward, done, info = env.step(action)  \n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            \n",
    "            # 한 에피소드가 게임 중에 끝날 시 -10을 보상 값으로 함\n",
    "            reward = reward if not done else -10  \n",
    "\n",
    "            # 샘플을 리플레이 메모리에 저장\n",
    "            agent.remember(state, action, reward, next_state, done)  \n",
    "\n",
    "            # 메모리의 샘플이 정의한 threshold보다 많으면 모델 훈련\n",
    "            if len(agent.memory) >= agent.train_threshold:  \n",
    "                agent.train_model()\n",
    "\n",
    "            score += reward\n",
    "            state = next_state  # 다음 상태의 값을 현제 상태의 값으로 변경\n",
    "\n",
    "            if done:  # 한 에피소드가 끝날 때마다 출력\n",
    "                toc = time.time()\n",
    "                agent.update_target_model()  # Q 함수 업데이트\n",
    "\n",
    "                print(\"Episode: {}/{}, Score: {}, time: {:.2f}s\".format(\n",
    "                    e, EPISODES, score, toc-tic))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
